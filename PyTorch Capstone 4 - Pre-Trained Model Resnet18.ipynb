{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "1Uqyyf55lxJ8"
            },
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "0rPFbF4plxKB"
            },
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "lCpk1VAwlxKF"
            },
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "xjZML1cQlxKI"
            },
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "OVfuRzxPlxKN"
            },
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "BHRgnUvDlxKQ"
            },
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n\n<ul>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>120 min</strong></p>\n </div>\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Zizypd1ulxKT"
            },
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "oyg35YcylxKW"
            },
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 222
                },
                "colab_type": "code",
                "id": "AiFYXJzglxKa",
                "outputId": "b18a8ab0-01c7-4add-d489-b68963dbee69"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-24 12:11:55--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\nPositive_tensors.zi 100%[===================>]   2.42G  49.4MB/s    in 54s     \n\n2020-05-24 12:12:49 (46.3 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "tiO69CWKlxKw"
            },
            "outputs": [],
            "source": "!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 222
                },
                "colab_type": "code",
                "id": "59QlE5XulxLA",
                "outputId": "292c7829-1e6f-4772-cbc8-5fb99246761f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-24 12:15:54--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\nNegative_tensors.zi 100%[===================>]   1.97G  50.7MB/s    in 44s     \n\n2020-05-24 12:16:39 (45.4 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Qjl70pKolxLO"
            },
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 101
                },
                "colab_type": "code",
                "id": "uFZ8bqmGlxLR",
                "outputId": "77a7f397-1893-49da-f139-5b246140b198"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\nRequirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.5.0+cu101)\nRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.4)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchvision) (0.16.0)\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "wzxTfWGclxLh"
            },
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "1iWOaH5slxLk"
            },
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "0rC1K9fflxLn",
                "outputId": "6d310b63-566a-4ed2-8f73-f08b0004654c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7fe89f0a6a90>"
                    },
                    "execution_count": 5,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "wftGSpsGlxL3"
            },
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "mLcuhF8elxMF"
            },
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Ccn6GbB6lxMI"
            },
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "jJ7inrCZlxMK"
            },
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "UGI8_fnElxMN",
                "outputId": "24d29750-80a3-4f05-dfee-c0f307ba050a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/content\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "O5Yd16XMlxMb"
            },
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "YXvO6ZmHlxMe",
                "outputId": "4234e9c1-5cc8-4a63-d9a1-fd0e868f9797"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "_ip-DNuClxMt"
            },
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "vMeyR5R3lxM1"
            },
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "jfRsecbZlxM5"
            },
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000,
                    "referenced_widgets": [
                        "ef64fa92b8d140739a30dfd90cbc509d",
                        "497baccba03043009cf84f1a9f69693b",
                        "9ab2c74a698b4607b35c175853dd4759",
                        "b39a832de7704ea0be3df88bf4dc444d",
                        "c0ff20e765224c9f85909ae79dc20152",
                        "cfb8540d0603414aa7564318661fd204",
                        "f548104beeaa47bf8d92e922db05324f",
                        "f91953b7da234f6bb270bbca7474a7dd"
                    ]
                },
                "colab_type": "code",
                "id": "9af-vC-DlxM9",
                "outputId": "884e3ac2-13ec-417a-b8d4-c938ed93dcfe"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ef64fa92b8d140739a30dfd90cbc509d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
                    },
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                },
                {
                    "data": {
                        "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"
                    },
                    "execution_count": 9,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\n\n# Type your code here\nmodel = models.resnet18(pretrained = True)\nmodel"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "mFBqXmXflxNN"
            },
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "yO9eU6xJlxNQ"
            },
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\n\n\n# Type your code here\nfor param in model.parameters():\n    param.requires_grad = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "rgFNWV26lxNe"
            },
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "g36GZ9S9lxNg"
            },
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "22ga9htGlxNj"
            },
            "outputs": [],
            "source": "model.fc = nn.Linear(512, 2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Ss6YHltrlxNy"
            },
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "colab_type": "code",
                "id": "1oYnDpEXlxN1",
                "outputId": "bcb494fe-3f10-4183-a8e1-cc97d6dedc53"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "RvLCFupwlxOC"
            },
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "M__tODP9lxOF"
            },
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "TJcGqV4ilxOJ"
            },
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "paCvK3LllxOL"
            },
            "outputs": [],
            "source": "# Step 1: Create the loss function\n\n# Type your code here\ncriterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "YmNeELJplxOY"
            },
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "_zLc2uLalxOa"
            },
            "outputs": [],
            "source": "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 100)\nvalid_loader = torch.utils.data.DataLoader(dataset = validation_dataset, batch_size = 100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Ttw9bh40lxOn"
            },
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "aeytsA_BlxOp"
            },
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "klV-LhIFlxOz"
            },
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "FEHclZLrlxO0"
            },
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "6pEZA0wXlxO2"
            },
            "outputs": [],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\n\nLoss=0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n\n        model.train() \n        \n        #clear gradient\n        optimizer.zero_grad()\n     \n        #make a prediction \n        y_hat = model(x)\n   \n        # calculate loss \n        loss = criterion(y_hat, y)\n    \n        # calculate gradients of parameters \n        loss.backward()\n        \n        # update parameters \n        optimizer.step()\n        \n        loss_list.append(loss.data)\n        \n    correct=0\n    \n    for x_test, y_test in valid_loader:\n        # set model to eval \n        model.eval()\n       \n        #make a prediction \n        y_hat = model(x_test)\n        \n        #find max \n        _, y_hat = torch.max(y_hat.data, 1)      \n       \n        #Calculate misclassified  samples in mini-batch \n        #hint +=(yhat==y_test).sum().item()\n        correct += (y_hat == y_test).sum().item()\n        \n   \n    accuracy=correct/N_test\n    loss_list.append(loss.data)\n    accuracy_list.append(accuracy)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "-zfasDLXlxO_"
            },
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "xZDBiLKAlxPB",
                "outputId": "49e02521-8827-4e8f-9ca4-16d9ed4e268c"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9929"
                    },
                    "execution_count": 17,
                    "metadata": {
                        "tags": []
                    },
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 279
                },
                "colab_type": "code",
                "id": "6zChPzQhlxPG",
                "outputId": "7ae4d419-d6e7-4717-90c7-d9da8a068a7b"
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light",
                        "tags": []
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "cwATHPTSlxPM"
            },
            "source": "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "YBWEd2xSlxPP"
            },
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 84
                },
                "colab_type": "code",
                "id": "hB4eCOsNlxPS",
                "outputId": "1f5d14a9-7744-4d06-87d8-d62ae3f6fb0d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Sample : 108; Expected Label: tensor([1]); Predicted Label: tensor([0])\nSample : 457; Expected Label: tensor([0]); Predicted Label: tensor([1])\nSample : 571; Expected Label: tensor([0]); Predicted Label: tensor([1])\nSample : 684; Expected Label: tensor([1]); Predicted Label: tensor([0])\n"
                }
            ],
            "source": "count = 0\n\ntest_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n\nfor i, (x_test, y_test) in enumerate(test_loader):\n    \n    model.eval()\n    \n    z = model(x_test)\n    \n    _, yhat = torch.max(z.data, 1)\n    \n    if yhat != y_test:\n        print(\"Sample : {}; Expected Label: {}; Predicted Label: {}\".format(str(i), str(y_test), str(yhat)))\n        \n        count += 1\n        if count >= 4:\n            break"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "U2yHcFq6lxPZ"
            },
            "source": "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "V6cPCIpNlxPa"
            },
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "XohCywmPlxPb"
            },
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [],
            "name": "PyTorch Capstone 4 - Pre-Trained Model Resnet18.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "497baccba03043009cf84f1a9f69693b": {
                    "model_module": "@jupyter-widgets/base",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "9ab2c74a698b4607b35c175853dd4759": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "100%",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_cfb8540d0603414aa7564318661fd204",
                        "max": 46827520,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_c0ff20e765224c9f85909ae79dc20152",
                        "value": 46827520
                    }
                },
                "b39a832de7704ea0be3df88bf4dc444d": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_f91953b7da234f6bb270bbca7474a7dd",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_f548104beeaa47bf8d92e922db05324f",
                        "value": " 44.7M/44.7M [1:00:13&lt;00:00, 13.0kB/s]"
                    }
                },
                "c0ff20e765224c9f85909ae79dc20152": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": "initial"
                    }
                },
                "cfb8540d0603414aa7564318661fd204": {
                    "model_module": "@jupyter-widgets/base",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "ef64fa92b8d140739a30dfd90cbc509d": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_9ab2c74a698b4607b35c175853dd4759",
                            "IPY_MODEL_b39a832de7704ea0be3df88bf4dc444d"
                        ],
                        "layout": "IPY_MODEL_497baccba03043009cf84f1a9f69693b"
                    }
                },
                "f548104beeaa47bf8d92e922db05324f": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "f91953b7da234f6bb270bbca7474a7dd": {
                    "model_module": "@jupyter-widgets/base",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}